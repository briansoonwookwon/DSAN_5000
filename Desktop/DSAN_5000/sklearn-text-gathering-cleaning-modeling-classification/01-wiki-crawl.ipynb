{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia Crawl example\n",
    "\n",
    "Author: J. Hickman\n",
    "\n",
    "- This code crawls through wikipedia to get a bunch of text data\n",
    "- The code lets the user specify search category topics.\n",
    "  - The more different the topics are, the easier the classification will be.\n",
    "  - For example, i used (pizza, metallurgy, basketball)\n",
    "- It then searches wikipedia for articles related to these topics\n",
    "- Loops over the wikipedia pages and gets the text from the wikipedia pages\n",
    "- Breaks the text into chunks (based on a user input specifying the number of sentences per chunk)\n",
    "- Each chunk is cleaned and tagged with a \"label\" (classification) and a numeric \"sentiment score\" (regression)\n",
    "- These cleaned chunks form a corpus of strings with associated tags\n",
    "\n",
    "```\n",
    "python -m pip install wikipedia_sections\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge wikipedia\n",
    "# conda install -c conda-forge wordcloud\n",
    "# python -m pip install wikipedia_sections\n",
    "\n",
    "import wikipedia\n",
    "import nltk\n",
    "import string \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/briankwon/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/briankwon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/briankwon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/briankwon/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THE FOLLOWING IF YOU HAVEN'T DOWNLOADED THESE BEFORE\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set user parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS \n",
    "label_list=['pizza','metallurgy','basketball']\n",
    "max_num_pages=25\n",
    "sentence_per_chunk=5\n",
    "min_sentence_length=20\n",
    "\n",
    "# GET STOPWORDS\n",
    "# from nltk.corpus import stopwords\n",
    "stop_words=nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# INITALIZE STEMMER+LEMITZIZER+SIA\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define text cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(text):\n",
    "\t# #FILTER OUT UNWANTED CHAR\n",
    "\tnew_text=\"\"\n",
    "\t# keep=string.printable\n",
    "\tkeep=\" abcdefghijklmnopqrstuvwxyz0123456789\"\n",
    "\tfor character in text:\n",
    "\t\tif character.lower() in keep:\n",
    "\t\t\tnew_text+=character.lower()\n",
    "\t\telse: \n",
    "\t\t\tnew_text+=\" \"\n",
    "\ttext=new_text\n",
    "\t# print(text)\n",
    "\n",
    "\t# #FILTER OUT UNWANTED WORDS\n",
    "\tnew_text=\"\"\n",
    "\tfor word in nltk.tokenize.word_tokenize(text):\n",
    "\t\tif word not in nltk.corpus.stopwords.words('english'):\n",
    "\t\t\t#lemmatize \n",
    "\t\t\ttmp=lemmatizer.lemmatize(word)\n",
    "\t\t\t# tmp=stemmer.stem(tmp)\n",
    "\n",
    "\t\t\t# update word if there is a change\n",
    "\t\t\t# if(tmp!=word): print(tmp,word)\n",
    "\t\t\t\n",
    "\t\t\tword=tmp\n",
    "\t\t\tif len(word)>1:\n",
    "\t\t\t\tif word in [\".\",\",\",\"!\",\"?\",\":\",\";\"]:\n",
    "\t\t\t\t\t#remove the last space\n",
    "\t\t\t\t\tnew_text=new_text[0:-1]+word+\" \"\n",
    "\t\t\t\telse: #add a space\n",
    "\t\t\t\t\tnew_text+=word.lower()+\" \"\n",
    "\ttext=new_text.strip()\n",
    "\treturn text\n",
    "\n",
    "# clean_string('the word \"pizza\" first appeared in a Latin text from the town of Gaeta, then still part of the Byzantine Empire, in 997 AD; the text states that a tenant of certain property is to give the bishop of Gaeta duodecim pizze (\"twelve pizzas\") every Christmas Day, and another twelve every Easter Sunday.Suggested etymologies include:')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preform a wikipedia crawl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages for label = pizza : ['Pizza', 'Pizza Pizza', 'Chicago-style pizza', 'Hawaiian pizza', 'Pizza Hut', \"Domino's\", 'History of pizza', 'Neapolitan pizza', \"Papa John's\", 'Sicilian pizza', 'Pizza Margherita', 'Pizza Tower', \"Marco's Pizza\", 'Licorice Pizza', 'Pizza delivery', 'Two Guys and a Girl', 'Pizza (disambiguation)', \"Shakey's Pizza\", 'Pizza (2012 film)', 'Mystic Pizza', 'Boston Pizza', 'Chuck E. Cheese', 'List of pizza chains of the United States', 'Blaze Pizza', '&pizza']\n",
      "\t Pizza\n",
      "\t Pizza Pizza\n",
      "\t Chicago-style pizza\n",
      "\t Hawaiian pizza\n",
      "\t Pizza Hut\n",
      "WARNING: SOMETHING WENT WRONG: Pizza Hut\n",
      "\t Domino's\n",
      "\t History of pizza\n",
      "\t Neapolitan pizza\n",
      "\t Papa John's\n",
      "\t Sicilian pizza\n",
      "\t Pizza Margherita\n",
      "\t Pizza Tower\n",
      "\t Marco's Pizza\n",
      "\t Licorice Pizza\n",
      "\t Pizza delivery\n",
      "\t Two Guys and a Girl\n",
      "WARNING: SOMETHING WENT WRONG: Two Guys and a Girl\n",
      "\t Pizza (disambiguation)\n",
      "WARNING: SOMETHING WENT WRONG: Pizza (disambiguation)\n",
      "\t Shakey's Pizza\n",
      "WARNING: SOMETHING WENT WRONG: Shakey's Pizza\n",
      "\t Pizza (2012 film)\n",
      "\t Mystic Pizza\n",
      "\t Boston Pizza\n",
      "\t Chuck E. Cheese\n",
      "\t List of pizza chains of the United States\n",
      "\t Blaze Pizza\n",
      "\t &pizza\n",
      "Pages for label = metallurgy : ['Metallurgy', 'Tempering (metallurgy)', 'Flux (metallurgy)', 'Ferrous metallurgy', 'Bronze', 'Titanium', 'Powder metallurgy', 'University of Leoben', 'Tin', 'Chemical metallurgy', 'Scythian metallurgy', 'Poling (metallurgy)', 'Annealing (materials science)', 'Nickel', 'Non-ferrous metal', 'Metallurgy of Russia', 'Recrystallization (metallurgy)', 'Iron Age', 'Beryllium', 'Physical metallurgy', 'Refining (metallurgy)', 'Iron metallurgy in Africa', 'Carbonyl metallurgy', 'Zinc', 'Plutonium']\n",
      "\t Metallurgy\n",
      "\t Tempering (metallurgy)\n",
      "\t Flux (metallurgy)\n",
      "\t Ferrous metallurgy\n",
      "\t Bronze\n",
      "\t Titanium\n",
      "WARNING: SOMETHING WENT WRONG: Titanium\n",
      "\t Powder metallurgy\n",
      "\t University of Leoben\n",
      "\t Tin\n",
      "WARNING: SOMETHING WENT WRONG: Tin\n",
      "\t Chemical metallurgy\n",
      "\t Scythian metallurgy\n",
      "\t Poling (metallurgy)\n",
      "\t Annealing (materials science)\n",
      "\t Nickel\n",
      "WARNING: SOMETHING WENT WRONG: Nickel\n",
      "\t Non-ferrous metal\n",
      "\t Metallurgy of Russia\n",
      "\t Recrystallization (metallurgy)\n",
      "\t Iron Age\n",
      "\t Beryllium\n",
      "\t Physical metallurgy\n",
      "\t Refining (metallurgy)\n",
      "\t Iron metallurgy in Africa\n",
      "\t Carbonyl metallurgy\n",
      "\t Zinc\n",
      "\t Plutonium\n",
      "Pages for label = basketball : ['Basketball', 'Basketball at the Asian Games', 'National Basketball Association', 'Basketball at the 2018 Asian Games – Men', 'Basketball at the SEA Games', \"Thailand men's national basketball team\", 'List of oldest and youngest National Basketball Association players', 'Basketball at the 2024 Summer Olympics', 'List of basketball films', 'List of basketball arenas', 'Basketball (ball)', 'College basketball', 'Basketball positions', 'Saudi Basketball League', 'FIBA', \"Philippines men's national basketball team\", 'History of basketball', 'FIBA Under-19 Basketball World Cup', \"Jordan men's national basketball team\", 'List of Olympic medalists in basketball', 'Basketball Wives', 'Basketball at the 1951 Asian Games', 'Basketball at the 1962 Asian Games', \"Hong Kong men's national basketball team\", \"Women's National Basketball Association\"]\n",
      "\t Basketball\n",
      "\t Basketball at the Asian Games\n",
      "\t National Basketball Association\n",
      "\t Basketball at the 2018 Asian Games – Men\n",
      "\t Basketball at the SEA Games\n",
      "\t Thailand men's national basketball team\n",
      "WARNING: SOMETHING WENT WRONG: Thailand men's national basketball team\n",
      "\t List of oldest and youngest National Basketball Association players\n",
      "\t Basketball at the 2024 Summer Olympics\n",
      "WARNING: SOMETHING WENT WRONG: Basketball at the 2024 Summer Olympics\n",
      "\t List of basketball films\n",
      "\t List of basketball arenas\n",
      "\t Basketball (ball)\n",
      "\t College basketball\n",
      "\t Basketball positions\n",
      "\t Saudi Basketball League\n",
      "\t FIBA\n",
      "WARNING: SOMETHING WENT WRONG: FIBA\n",
      "\t Philippines men's national basketball team\n",
      "\t History of basketball\n",
      "\t FIBA Under-19 Basketball World Cup\n",
      "\t Jordan men's national basketball team\n",
      "\t List of Olympic medalists in basketball\n",
      "\t Basketball Wives\n",
      "\t Basketball at the 1951 Asian Games\n",
      "\t Basketball at the 1962 Asian Games\n",
      "\t Hong Kong men's national basketball team\n",
      "\t Women's National Basketball Association\n"
     ]
    }
   ],
   "source": [
    "#INITIALIZE \n",
    "corpus=[]  # list of strings (input variables X)\n",
    "targets=[] # list of targets (labels or response variables Y)\n",
    "\n",
    "#--------------------------\n",
    "# LOOP OVER TOPICS \n",
    "#--------------------------\n",
    "for label in label_list:\n",
    "\n",
    "\t#SEARCH FOR RELEVANT PAGES \n",
    "\ttitles=wikipedia.search(label,results=max_num_pages)\n",
    "\tprint(\"Pages for label =\",label,\":\",titles)\n",
    "\n",
    "\t#LOOP OVER WIKI-PAGES\n",
    "\tfor title in titles:\n",
    "\t\ttry:\n",
    "\t\t\tprint(\"\t\",title)\n",
    "\t\t\twiki_page = wikipedia.page(title, auto_suggest=True)\n",
    "\n",
    "\t\t\t# LOOP OVER SECTIONS IN ARTICLE AND GET PAGE TEXT\n",
    "\t\t\tfor section in wiki_page.sections:\n",
    "\t\t\t\ttext=wiki_page.section(section); #print(text)\n",
    "\n",
    "\t\t\t\t#BREAK IN TO SENTANCES \n",
    "\t\t\t\tsentences=nltk.tokenize.sent_tokenize(text)\n",
    "\t\t\t\tcounter=0\n",
    "\t\t\t\ttext_chunk=''\n",
    "\n",
    "\t\t\t\t#LOOP OVER SENTENCES \n",
    "\t\t\t\tfor sentence in sentences:\n",
    "\t\t\t\t\tif len(sentence)>min_sentence_length:\n",
    "\t\t\t\t\t\tif(counter%sentence_per_chunk==0 and counter!=0):\n",
    "\t\t\t\t\t\t\t# PROCESS COMPLETED CHUNK \n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t# CLEAN STRING\n",
    "\t\t\t\t\t\t\ttext_chunk=clean_string(text_chunk)\n",
    "\n",
    "\t\t\t\t\t\t\t# REMOVE LABEL IF IN STRING (MAKES IT TOO EASY)\n",
    "\t\t\t\t\t\t\ttext_chunk=text_chunk.replace(label,\"\")\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t# REMOVE ANY DOUBLE SPACES\n",
    "\t\t\t\t\t\t\ttext_chunk=' '.join(text_chunk.split()).strip()\n",
    "\n",
    "\t\t\t\t\t\t\t#UPDATE CORPUS \n",
    "\t\t\t\t\t\t\tcorpus.append(text_chunk)\n",
    "\n",
    "\t\t\t\t\t\t\t#UPDATE TARGETS\n",
    "\t\t\t\t\t\t\tscore=sia.polarity_scores(text_chunk)\n",
    "\t\t\t\t\t\t\ttarget=[label,score['compound']]\n",
    "\t\t\t\t\t\t\ttargets.append(target)\n",
    "\n",
    "\t\t\t\t\t\t\t#print(\"TEXT\\n\",text_chunk,target)\n",
    "\n",
    "\t\t\t\t\t\t\t# RESET CHUNK FOR NEXT ITERATION \n",
    "\t\t\t\t\t\t\ttext_chunk=sentence\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\ttext_chunk+=sentence\n",
    "\t\t\t\t\t\t#print(\"--------\\n\", sentence)\n",
    "\t\t\t\t\t\tcounter+=1\n",
    "\n",
    "\t\texcept:\n",
    "\t\t\tprint(\"WARNING: SOMETHING WENT WRONG:\", title);  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of text chunks =  5693\n",
      "number of targets =  5693\n",
      "                                                   text       label  sentiment\n",
      "0     word first appeared latin text town gaeta stil...       pizza     0.2732\n",
      "1     modern greek pitta bread apulia calabrian byza...       pizza     0.0000\n",
      "2     word pitta turn traced either ancient greek pi...       pizza     0.0000\n",
      "3     etymological dictionary italian language expla...       pizza     0.0000\n",
      "4                      origin latin pinsere pound stamp       pizza     0.0000\n",
      "...                                                 ...         ...        ...\n",
      "5688  katrina mcclain award first presented 2018 top...  basketball     0.6486\n",
      "5689  lisa leslie award first presented 2018 top cen...  basketball     0.9413\n",
      "5690  hall also formerly presented france pomeroy na...  basketball     0.8779\n",
      "5691  men award given since 1969 voted national asso...  basketball     0.5423\n",
      "5692                                 sporting news 2005  basketball     0.0000\n",
      "\n",
      "[5693 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#SANITY CHECKS AND PRINT TO FILE \n",
    "print(\"number of text chunks = \",len(corpus))\n",
    "print(\"number of targets = \",len(targets))\n",
    "\n",
    "tmp=[]\n",
    "for i in range(0,len(corpus)):\n",
    "    tmp.append([corpus[i],targets[i][0],targets[i][1]])\n",
    "df=pd.DataFrame(tmp)\n",
    "df=df.rename(columns={0: \"text\", 1: \"label\", 2: \"sentiment\"})\n",
    "print(df)\n",
    "df.to_csv('wiki-crawl-results.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #RELOAD FILE AND PRETEND THAT IS OUR STARTING POINT \n",
    "# df=pd.read_csv('wiki-crawl-results.csv')  \n",
    "# #print(df)\n",
    "\n",
    "# #CONVERT FROM STRING LABELS TO INTEGERS \n",
    "# labels=[]; #y1=[]; y2=[]\n",
    "# y1=[]\n",
    "# for label in df[\"label\"]:\n",
    "#     if label not in labels:\n",
    "#         labels.append(label)\n",
    "#         print(\"index =\",len(labels)-1,\": label =\",label)\n",
    "#     for i in range(0,len(labels)):\n",
    "#         if(label==labels[i]):\n",
    "#             y1.append(i)\n",
    "# y1=np.array(y1)\n",
    "\n",
    "# # CONVERT DF TO LIST OF STRINGS \n",
    "# corpus=df[\"text\"].to_list()\n",
    "# y2=df[\"sentiment\"].to_numpy()\n",
    "\n",
    "# print(\"number of text chunks = \",len(corpus))\n",
    "# print(len(y1))\n",
    "# print(corpus[0:3])\n",
    "\n",
    "# # INITIALIZE COUNT VECTORIZER\n",
    "# vectorizer=CountVectorizer()   \n",
    "\n",
    "# # RUN COUNT VECTORIZER ON OUR COURPUS \n",
    "# Xs  =  vectorizer.fit_transform(corpus)   \n",
    "# X=np.array(Xs.todense())\n",
    "\n",
    "# #CONVERT TO ONE-HOT VECTORS\n",
    "# maxs=np.max(X,axis=0)\n",
    "# X=np.ceil(X/maxs)\n",
    "\n",
    "# # DOUBLE CHECK \n",
    "# print(X.shape,y1.shape,y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ANLY501')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3bbb781ca6673b7d7a2eaec0820775eebfaab6ec1fac7365fb415515f8c23aa5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
